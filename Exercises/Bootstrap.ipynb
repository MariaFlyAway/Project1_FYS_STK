{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrankeFunction(x,y):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return term1 + term2 + term3 + term4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.24652348154963583\n",
      "Bias^2: 0.2441326021841261\n",
      "Var: 0.0023908793655097074\n",
      "0.24652348154963583 >= 0.2441326021841261 + 0.0023908793655097074 = 0.24652348154963583\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonas\\anaconda3\\envs\\fysstk3155\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.635e+01, tolerance: 6.700e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "n_boostraps = 100\n",
    "degree = 5\n",
    "noise = 0.1\n",
    "\n",
    "# Make data.\n",
    "n = 100\n",
    "x = np.linspace(0, 1, n)\n",
    "y = np.linspace(0, 1, n)\n",
    "x, y = np.meshgrid(x,y)\n",
    "\n",
    "# we unravel the values to change the shape from (100, 100) to (10000, )\n",
    "x = x.ravel()\n",
    "y = y.ravel()\n",
    "\n",
    "k = .01 # noise coefficient \n",
    "z = FrankeFunction(x, y) # + k*np.random.randn(n^2)\n",
    "\n",
    "X = np.column_stack((x,y))\n",
    "\n",
    "# Hold out some test data that is never used in training.\n",
    "X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.2, random_state=3)\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        StandardScaler(),\n",
    "        Lasso(alpha=1e-4))\n",
    "\n",
    "# The following (m x n_bootstraps) matrix holds the column vectors y_pred for each bootstrap iteration.\n",
    "z_pred = np.empty((z_test.shape[0], n_boostraps))\n",
    "for i in range(n_boostraps):\n",
    "    x_, y_ = resample(X_train, z_train) # bootstrap from sklearn.utils\n",
    "\n",
    "    # Evaluate the new model on the same test data each time.\n",
    "z_pred[:, i] = pipeline.fit(x_, y_).predict(X_test).ravel()\n",
    "\n",
    "z_test = z_test.reshape((2000, 1))\n",
    "\n",
    "# Note: Expectations and variances taken w.r.t. different training\n",
    "# data sets, hence the axis=1. Subsequent means are taken across the test data\n",
    "# set in order to obtain a total value, but before this we have error/bias/variance\n",
    "# calculated per data point in the test set.\n",
    "# Note 2: The use of keepdims=True is important in the calculation of bias as this\n",
    "# maintains the column vector form. Dropping this yields very unexpected results.\n",
    "error = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )\n",
    "bias = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )\n",
    "variance = np.mean( np.var(z_pred, axis=1, keepdims=True) )\n",
    "print('Error:', error)\n",
    "print('Bias^2:', bias)\n",
    "print('Var:', variance)\n",
    "print(f'{error} >= {bias} + {variance} = {bias+variance}')\n",
    "print(f'{error - (bias+variance)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fysstk3155",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
