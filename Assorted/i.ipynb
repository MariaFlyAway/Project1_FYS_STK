{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1\n",
    "\n",
    "### a)\n",
    "- Generate data from\n",
    "- Implement Franke function with noise\n",
    "- Write code for OLS up to fifth order - calculate MSE and R2\n",
    "- Center and split data - discussion of how, why/why not for scaling\n",
    "- Plot MSE and R2 \n",
    "- Plot $\\beta$-parameters - comment\n",
    "\n",
    "### b)\n",
    "\n",
    "- Implement Ridge method with Franke function\n",
    "- Perform same analysis as above with different $\\lambda$-values - study dependence\n",
    "- Compare with a\n",
    "\n",
    "### c)\n",
    "\n",
    "- Implement Lasso method\n",
    "- iterate over different lambda values\n",
    "- note the intercept - scikit excludes\n",
    "- discuss the three models and which fits the data best\n",
    "\n",
    "### d)\n",
    "\n",
    "- Derive the expected value and variance - done, needs explanations\n",
    "- Part of theory section \n",
    "\n",
    "### e)\n",
    "\n",
    "- make fig 2.11 Hastie\n",
    "- derive bias-variance tradeoff formula - include in theory-section\n",
    "- explain what the terms mean and discuss their interpretations\n",
    "- Perform a bias-variance analysis of the Franke by studying the MSE value as a function of the model complexity\n",
    "and the number of datapoints and \n",
    "- bootstrap-implementation - used for analysis and discussion\n",
    "\n",
    "### f)\n",
    "\n",
    "- implement k-fold cross validation - scikit-learn - 5-10 folds\n",
    "- evaluate the MSE function resulting from the test folds\n",
    "- compare the MSE from cross-validation to MSE from bootstrap, comment results\n",
    "- Ridge, Lasso and OLS\n",
    "\n",
    "### g)\n",
    "\n",
    "- download data\n",
    "- prepare inputs to code\n",
    "\n",
    "\n",
    "## Report\n",
    "\n",
    "__Figures and data to be included in report:__\n",
    "\n",
    "### a)\n",
    "\n",
    "__Results__: MSE and R2-score as a function of model compelxity for OLS<br>\n",
    "__Discussion__: Critical discussion of why we chose to sclae/not scale our data\n",
    "\n",
    "### b) \n",
    "\n",
    "__Results__: Same analysis as above but for different $\\lambda$-values<br>\n",
    "__Discussion__: Compare results with those from a). Study the dependence on $\\lambda$\n",
    "\n",
    "### c)\n",
    "\n",
    "__Results__: Same analysis as above but for different $\\lambda$-values <br>\n",
    "__Discussion__: Compare results with those from a). Study the dependence on $\\lambda$. Give a critical discussion of the three methods and a judgement of which model fits the data best. \n",
    "\n",
    "### d)\n",
    "\n",
    "__Theory__: derivation of various properties\n",
    "\n",
    "### e)\n",
    "\n",
    "__Theory__: derivation of bias-variance tradeoff<br>\n",
    "__Results__: reproduce figure 2.11 from Hastie, Tibshirani and Friedman. Bias-variance analysis of the Franke Function by studying the MSE value as a function of the complexity of the model.<br>\n",
    "__Discussion__: discuss the bias and variance tradeoff as a function of the model complexity, number of datapoints and training and test data using the bootstrap resampling method. \n",
    "\n",
    "### f)\n",
    "\n",
    "__Results__: MSE as a function resulting from the test folds<br>\n",
    "__Discussion__: compare results from bootstrap, use 5-10 folds - OLS, Ridge and Lasso\n",
    "\n",
    "### g)\n",
    "\n",
    "All of the above\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
